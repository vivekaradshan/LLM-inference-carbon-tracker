{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161280bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:52:36] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:52:36] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:52:36] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:52:36] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:52:36] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:52:37] CPU Model on constant consumption mode: AMD Ryzen 5 4600H with Radeon Graphics\n",
      "[codecarbon INFO @ 12:52:37] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:52:37]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 12:52:37]   Python version: 3.9.13\n",
      "[codecarbon INFO @ 12:52:37]   CodeCarbon version: 2.5.0\n",
      "[codecarbon INFO @ 12:52:37]   Available RAM : 15.372 GB\n",
      "[codecarbon INFO @ 12:52:37]   CPU count: 12\n",
      "[codecarbon INFO @ 12:52:37]   CPU model: AMD Ryzen 5 4600H with Radeon Graphics\n",
      "[codecarbon INFO @ 12:52:37]   GPU count: 1\n",
      "[codecarbon INFO @ 12:52:37]   GPU model: 1 x NVIDIA GeForce GTX 1650\n",
      "[codecarbon INFO @ 12:52:38] Saving emissions data to file C:\\Users\\Viveka\\emissions.csv\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 12:52:39] Energy consumed for RAM : 0.000003 kWh. RAM Power : 5.764642238616943 W\n",
      "[codecarbon INFO @ 12:52:39] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 6.680138277976911 W\n",
      "[codecarbon INFO @ 12:52:39] Energy consumed for all CPUs : 0.000013 kWh. Total CPU Power : 27.0 W\n",
      "[codecarbon INFO @ 12:52:39] 0.000018 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain bayes theorem with example.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1562663939372121e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Initialize the EmissionsTracker\n",
    "tracker = EmissionsTracker()\n",
    "\n",
    "# Start tracking\n",
    "tracker.start()\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a smaller model and tokenizer\n",
    "model_name = \"distilgpt2\"  # Using a smaller model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=50, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model\n",
    "prompt = \"explain bayes theorem with example\"\n",
    "print(generate_text(prompt))\n",
    "tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c843252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:53:07] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:53:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:53:07] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:53:07] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:53:07] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:53:09] CPU Model on constant consumption mode: AMD Ryzen 5 4600H with Radeon Graphics\n",
      "[codecarbon INFO @ 12:53:09] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:53:09]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 12:53:09]   Python version: 3.9.13\n",
      "[codecarbon INFO @ 12:53:09]   CodeCarbon version: 2.5.0\n",
      "[codecarbon INFO @ 12:53:09]   Available RAM : 15.372 GB\n",
      "[codecarbon INFO @ 12:53:09]   CPU count: 12\n",
      "[codecarbon INFO @ 12:53:09]   CPU model: AMD Ryzen 5 4600H with Radeon Graphics\n",
      "[codecarbon INFO @ 12:53:09]   GPU count: 1\n",
      "[codecarbon INFO @ 12:53:09]   GPU model: 1 x NVIDIA GeForce GTX 1650\n",
      "[codecarbon INFO @ 12:53:09] Saving emissions data to file C:\\Users\\Viveka\\emissions.csv\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for RAM : 0.000003 kWh. RAM Power : 5.764642238616943 W\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 7.891991419600592 W\n",
      "[codecarbon INFO @ 12:53:11] Energy consumed for all CPUs : 0.000015 kWh. Total CPU Power : 27.0 W\n",
      "[codecarbon INFO @ 12:53:11] 0.000023 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain bayes theorem with example of the Bayesian model.\n",
      "\n",
      "The Bayesian model is a generalization of the Bayesian model. It is a generalization of the Bayesian model. It is a generalization of the Bayesian model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4568661368169474e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Initialize the EmissionsTracker\n",
    "tracker = EmissionsTracker()\n",
    "\n",
    "# Start tracking\n",
    "tracker.start()\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and quantized model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=50, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model\n",
    "prompt = \"explain bayes theorem with example\"\n",
    "print(generate_text(prompt))\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1bf49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
